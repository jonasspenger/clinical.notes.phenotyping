{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Implementation of the cTAKES BoW method with relation pairs (f.e. CUI-Relationship-CUI) (added to the BoW cTAKES orig. pairs (Polarity-CUI)), evaluated against the annotations from: \n",
    "> Gehrmann, Sebastian, et al. \"Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives.\" PloS one 13.2 (2018): e0192360."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported packages\n",
    "import multiprocessing\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "\n",
    "# xml and xmi\n",
    "from lxml import etree\n",
    "\n",
    "# arrays and dataframes\n",
    "import pandas\n",
    "import numpy\n",
    "from pandasql import sqldf\n",
    "\n",
    "# classifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# plotting\n",
    "import matplotlib \n",
    "matplotlib.use('Agg') # server\n",
    "try:\n",
    "    get_ipython\n",
    "    # jupyter notebook\n",
    "    %matplotlib inline \n",
    "except:\n",
    "    pass\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import custom modules\n",
    "import context # set search path to one level up\n",
    "from src import evaluation  # method for evaluation of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables and parameters\n",
    "# filenames\n",
    "input_directory = '../data/interim/cTAKES_output'\n",
    "input_filename = '../data/raw/annotations.csv'\n",
    "results_filename = '../reports/ctakes_relationgram_bow_tfidf_results.csv'\n",
    "plot_filename_1 = '../reports/ctakes_relationgram_bow_tfidf_boxplot_1.png'\n",
    "plot_filename_2 = '../reports/ctakes_relationgram_bow_tfidf_boxplot_2.png'\n",
    "\n",
    "# number of splits and repeats for cross validation\n",
    "n_splits = 5\n",
    "n_repeats = 10\n",
    "# n_repeats = 1  # for testing\n",
    "\n",
    "# number of workers\n",
    "n_workers=multiprocessing.cpu_count()\n",
    "# n_workers = 1  # for testing\n",
    "\n",
    "# keep the conditions for which results are reported in the publication\n",
    "conditions = [  \n",
    "#     'cohort',\n",
    "    'Obesity',\n",
    "#     'Non.Adherence',\n",
    "#     'Developmental.Delay.Retardation',\n",
    "    'Advanced.Heart.Disease', \n",
    "    'Advanced.Lung.Disease', \n",
    "    'Schizophrenia.and.other.Psychiatric.Disorders',\n",
    "    'Alcohol.Abuse', \n",
    "    'Other.Substance.Abuse',\n",
    "    'Chronic.Pain.Fibromyalgia', \n",
    "    'Chronic.Neurological.Dystrophies', \n",
    "    'Advanced.Cancer',\n",
    "    'Depression',\n",
    "#     'Dementia',\n",
    "#     'Unsure',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and parse xmi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache --read 2.6-JS-ctakes-relationgram-bow-tfidf_cache.pkl X  \n",
    "\n",
    "def ctakes_xmi_to_df(xmi_path):\n",
    "    records = []\n",
    "    \n",
    "    tree = etree.parse(xmi_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    mentions = []\n",
    "    for mention in root.iterfind('*[@{http://www.omg.org/XMI}id][@typeID][@polarity]'):\n",
    "        if 'ontologyConceptArr' in mention.attrib:\n",
    "            for concept in mention.attrib['ontologyConceptArr'].split(\" \"):\n",
    "                d = dict(mention.attrib)\n",
    "                d['ontologyConceptArr'] = concept\n",
    "                mentions.append(d)\n",
    "        else:\n",
    "            d = dict(mention.attrib)\n",
    "            mentions.append(d)\n",
    "    mentions_df = pandas.DataFrame(mentions)\n",
    "    \n",
    "    concepts = []\n",
    "    for concept in root.iterfind('*[@{http://www.omg.org/XMI}id][@cui][@tui]'):\n",
    "        concepts.append(dict(concept.attrib))\n",
    "    concepts_df = pandas.DataFrame(concepts)\n",
    "    \n",
    "    events = []\n",
    "    for event in root.iterfind('*[@{http://www.omg.org/XMI}id][@properties]'):\n",
    "        events.append(dict(event.attrib))\n",
    "    events_df = pandas.DataFrame(events)\n",
    "    \n",
    "    eventproperties = []\n",
    "    for eventpropertie in root.iterfind('*[@{http://www.omg.org/XMI}id][@docTimeRel]'):\n",
    "        eventproperties.append(dict(eventpropertie.attrib))\n",
    "    eventproperties_df = pandas.DataFrame(eventproperties)\n",
    "    \n",
    "    merged_df = mentions_df.add_suffix('_1')\\\n",
    "        .merge(right=concepts_df, left_on='ontologyConceptArr_1', right_on='{http://www.omg.org/XMI}id')\\\n",
    "        .merge(right=events_df, left_on='event_1', right_on='{http://www.omg.org/XMI}id')\\\n",
    "        .merge(right=eventproperties_df, left_on='properties', right_on='{http://www.omg.org/XMI}id')\n",
    "    \n",
    "#     # unique cui and tui per event IDEA: consider keeping all\n",
    "#     merged_df = merged_df.drop_duplicates(subset=['event', 'cui', 'tui'])\n",
    "    \n",
    "    # merge polarity of the *mention and the cui\n",
    "    merged_df = merged_df.dropna(subset=['cui'])  # remove any NaN\n",
    "    merged_df['polaritycui'] = merged_df['polarity_1'] + merged_df['cui']\n",
    "    \n",
    "    # extract relations\n",
    "    textrelations = []\n",
    "    for tr in root.iterfind('*[@{http://www.omg.org/XMI}id][@category][@arg1][@arg2]'):\n",
    "        textrelations.append(dict(tr.attrib))\n",
    "    textrelations_df = pandas.DataFrame(textrelations)\n",
    "    \n",
    "    relationarguments = []\n",
    "    for relationargument in root.iterfind('*[@{http://www.omg.org/XMI}id][@argument][@role]'):\n",
    "        relationarguments.append(dict(relationargument.attrib))\n",
    "    relationarguments_df = pandas.DataFrame(relationarguments)    \n",
    "    \n",
    "    # transforms\n",
    "    tdf = textrelations_df\n",
    "    tdf['xmiid'] = tdf['{http://www.omg.org/XMI}id']\n",
    "    rdf = relationarguments_df\n",
    "    rdf['xmiid'] = rdf['{http://www.omg.org/XMI}id']\n",
    "    mdf = mentions_df\n",
    "    mdf['xmiid'] = mdf['{http://www.omg.org/XMI}id']\n",
    "    cdf = concepts_df\n",
    "    cdf['xmiid'] = cdf['{http://www.omg.org/XMI}id']\n",
    "\n",
    "    subquery_1 = \"\"\"\n",
    "    -- table with:\n",
    "        -- (from *Relation): category\n",
    "        -- (from RelationArgument): argument (as argument1 and argument2) (Foreign Key *Mentions.xmiid)\n",
    "        -- (from *Mention): begin - end (as begin1 - end1 - begin2 - end2)\n",
    "        SELECT\n",
    "            r.category,\n",
    "            m1.begin as begin1,\n",
    "            m1.end as end1,\n",
    "            m2.begin as begin2,\n",
    "            m2.end as end2\n",
    "        FROM\n",
    "            tdf r\n",
    "        INNER JOIN\n",
    "            rdf a1\n",
    "            ON r.arg1 = a1.xmiid\n",
    "            INNER JOIN\n",
    "                rdf a2\n",
    "                ON r.arg2 = a2.xmiid\n",
    "                INNER JOIN\n",
    "                    mdf m1\n",
    "                    ON a1.argument = m1.xmiid\n",
    "                    INNER JOIN\n",
    "                        mdf m2\n",
    "                        ON a2.argument = m2.xmiid\n",
    "    \"\"\"\n",
    "\n",
    "    subquery_2 = \"\"\"\n",
    "    -- table with: \n",
    "        -- (from *Mentions): begin - end - polarity\n",
    "        -- (from Concepts): cui\n",
    "        SELECT\n",
    "            m.begin,\n",
    "            m.end,\n",
    "            m.polarity,\n",
    "            c.cui\n",
    "        FROM\n",
    "            mdf m\n",
    "            INNER JOIN\n",
    "            cdf c\n",
    "            ON\n",
    "            m.ontologyConceptArr = c.xmiid\n",
    "    \"\"\"\n",
    "\n",
    "    # run subqueries and save in new tables\n",
    "    sq1 = sqldf(subquery_1, locals())\n",
    "    sq2 = sqldf(subquery_2, locals())\n",
    "\n",
    "    query = \"\"\"\n",
    "    -- table with:\n",
    "    -- (from Concept): cui1, cui2\n",
    "    -- (from *Mention): polarity1, polarity2\n",
    "    -- (from *Relation): category (what kind of relation)\n",
    "    SELECT\n",
    "        sq1.category,\n",
    "        sq21.cui as cui1,\n",
    "        sq22.cui as cui2,\n",
    "        sq21.polarity as polarity1,\n",
    "        sq22.polarity as polarity2\n",
    "    FROM\n",
    "        sq1 sq1\n",
    "    INNER JOIN\n",
    "        sq2 sq21\n",
    "        ON sq21.begin >= sq1.begin1\n",
    "        and sq21.end <= sq1.end1\n",
    "        INNER JOIN\n",
    "            sq2 sq22\n",
    "            ON sq22.begin >= sq1.begin2\n",
    "            and sq22.end <= sq1.end2\n",
    "    \"\"\"\n",
    "\n",
    "    res = sqldf(query, locals())\n",
    "\n",
    "    # remove duplicates\n",
    "    res = res.drop_duplicates(subset=['cui1', 'cui2', 'category', 'polarity1', 'polarity2'])\n",
    "\n",
    "    res['string'] = res['polarity1'] + res['cui1'] + res['category'] + res['polarity2'] + res['cui2']\n",
    "\n",
    "    # return as a string\n",
    "    return ' '.join(list(res['string']) + list(merged_df['polaritycui']))\n",
    "\n",
    "X = []\n",
    "\n",
    "# key function for sorting the files according to the integer of the filename\n",
    "def key_fn(x):\n",
    "    i = x.split(\".\")[0]\n",
    "    if i != \"\":\n",
    "        return int(i)\n",
    "    return None\n",
    "\n",
    "for f in sorted(os.listdir(input_directory), key=key_fn):  # for each file in the input directory\n",
    "    if f.endswith(\".xmi\"):\n",
    "        fpath = os.path.join(input_directory, f)\n",
    "        # parse file and append as a dataframe to x_df\n",
    "        try:\n",
    "            X.append(ctakes_xmi_to_df(fpath))\n",
    "        except Exception as e:\n",
    "            print e\n",
    "            X.append('NaN')\n",
    "\n",
    "X = numpy.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load annotations and classification data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and parse csv file\n",
    "data = pandas.read_csv(input_filename)\n",
    "# data = data[0:100]  # for testing\n",
    "# X = X[0:100]  # for testing\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups: the subject ids\n",
    "# used in order to ensure that \n",
    "# \"patients’ notes stay within the set, so that all discharge notes in the \n",
    "# test set are from patients not previously seen by the model.\" Gehrmann17.\n",
    "groups_df = data.filter(items=['subject.id']) \n",
    "groups = groups_df.as_matrix()\n",
    "# y: the annotated classes\n",
    "y_df = data.filter(items=conditions) # filter the conditions\n",
    "y = y_df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, groups.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of classifiers (sklearn estimators)\n",
    "classifiers = collections.OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    pattern = r'[\\s]+'  # match any sequence of whitespace characters\n",
    "    repl = r' '  # replace with space\n",
    "    temp_text = re.sub(pattern, repl, text)\n",
    "    return temp_text.lower().split(' ')  # lower-case and split on space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_models = [\n",
    "    ('logistic_regression', LogisticRegression(random_state=0)),\n",
    "    (\"random_forest\", RandomForestClassifier(random_state=0)),\n",
    "    (\"naive_bayes\", MultinomialNB()),\n",
    "    (\"svm_linear\", SVC(kernel=\"linear\", random_state=0, probability=True)),\n",
    "    (\"gradient_boosting\", GradientBoostingClassifier(random_state=0)),\n",
    "]\n",
    "\n",
    "# BoW\n",
    "representation_models = [('ctakes_relationgram_bow_tfidf', TfidfVectorizer(tokenizer=tokenizer))]  # IDEA: Use Tfidf on normal BoW model aswell?\n",
    "\n",
    "# cross product of representation models and prediction models\n",
    "# save to classifiers as pipelines of rep. model into pred. model\n",
    "for rep_model, pred_model in itertools.product(representation_models, prediction_models):\n",
    "    classifiers.update({  # add this classifier to classifiers dictionary\n",
    "        '{rep_model}_{pred_model}'.format(rep_model=rep_model[0], pred_model=pred_model[0]):  # classifier name\n",
    "        Pipeline([rep_model, pred_model]),  # concatenate representation model with prediction model in a pipeline\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluation.run_evaluation(X=X, \n",
    "                                    y=y, \n",
    "                                    groups=groups,\n",
    "                                    conditions=conditions,\n",
    "                                    classifiers=classifiers,\n",
    "                                    n_splits=n_splits, \n",
    "                                    n_repeats=n_repeats, \n",
    "                                    n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results        \n",
    "results_df = pandas.DataFrame(results)\n",
    "results_df.to_csv(results_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load results for plotting\n",
    "# import pandas\n",
    "# results = pandas.read_csv('output/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save\n",
    "axs = results_df.groupby('name').boxplot(column='AUROC', by='condition', rot=90, figsize=(10,10))\n",
    "for ax in axs:\n",
    "    ax.set_ylim(0,1)\n",
    "\n",
    "plt.savefig(plot_filename_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save\n",
    "axs = results_df.groupby('condition').boxplot(column='AUROC', by='name', rot=90, figsize=(10,10))\n",
    "for ax in axs:\n",
    "    ax.set_ylim(0,1)\n",
    "\n",
    "plt.savefig(plot_filename_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
