{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNFINISHED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Reimplementation of the BoW (with n-grams) baseline methods presented in and evaluated against the annotations from: \n",
    "> Gehrmann, Sebastian, et al. \"Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives.\" PloS one 13.2 (2018): e0192360."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported packages\n",
    "import multiprocessing\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "\n",
    "# xml and xmi\n",
    "from lxml import etree\n",
    "\n",
    "# arrays and dataframes\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "# classifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# plotting\n",
    "import matplotlib \n",
    "matplotlib.use('Agg') # server\n",
    "try:\n",
    "    get_ipython\n",
    "    # jupyter notebook\n",
    "    %matplotlib inline \n",
    "except:\n",
    "    pass\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import custom modules\n",
    "import context # set search path to one level up\n",
    "from src import evaluation  # method for evaluation of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables and parameters\n",
    "# filenames\n",
    "input_directory = '../data/interim/cTAKES_output'\n",
    "input_filename = '../data/raw/annotations.csv'\n",
    "results_filename = '../reports/ctakes_timegram_bow_tfidf_results.csv'\n",
    "plot_filename_1 = '../reports/ctakes_timegram_bow_tfidf_boxplot_1.png'\n",
    "plot_filename_2 = '../reports/ctakes_timegram_bow_tfidf_boxplot_2.png'\n",
    "\n",
    "# number of splits and repeats for cross validation\n",
    "n_splits = 5\n",
    "n_repeats = 10\n",
    "n_repeats = 1  # for testing\n",
    "\n",
    "# number of workers\n",
    "n_workers=multiprocessing.cpu_count()\n",
    "# n_workers = 1  # for testing\n",
    "\n",
    "# keep the conditions for which results are reported in the publication\n",
    "conditions = [  \n",
    "#     'cohort',\n",
    "    'Obesity',\n",
    "#     'Non.Adherence',\n",
    "#     'Developmental.Delay.Retardation',\n",
    "    'Advanced.Heart.Disease', \n",
    "    'Advanced.Lung.Disease', \n",
    "    'Schizophrenia.and.other.Psychiatric.Disorders',\n",
    "    'Alcohol.Abuse', \n",
    "    'Other.Substance.Abuse',\n",
    "    'Chronic.Pain.Fibromyalgia', \n",
    "    'Chronic.Neurological.Dystrophies', \n",
    "    'Advanced.Cancer',\n",
    "    'Depression',\n",
    "#     'Dementia',\n",
    "#     'Unsure',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and parse xmi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache --read 2.5-JS-ctakes-timegram-bow-tfidf_cache.pkl X  \n",
    "\n",
    "def ctakes_xmi_to_df(xmi_path):\n",
    "    records = []\n",
    "    \n",
    "    tree = etree.parse(xmi_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    mentions = []\n",
    "    for mention in root.iterfind('*[@{http://www.omg.org/XMI}id][@typeID][@polarity][@ontologyConceptArr][@event]'):\n",
    "        for concept in mention.attrib['ontologyConceptArr'].split(\" \"):\n",
    "            d = dict(mention.attrib)\n",
    "            d['ontologyConceptArr'] = concept\n",
    "            mentions.append(d)\n",
    "    mentions_df = pandas.DataFrame(mentions)\n",
    "    \n",
    "    concepts = []\n",
    "    for concept in root.iterfind('*[@{http://www.omg.org/XMI}id][@cui][@tui]'):\n",
    "        concepts.append(dict(concept.attrib))\n",
    "    concepts_df = pandas.DataFrame(concepts)\n",
    "    \n",
    "    events = []\n",
    "    for event in root.iterfind('*[@{http://www.omg.org/XMI}id][@properties]'):\n",
    "        events.append(dict(event.attrib))\n",
    "    events_df = pandas.DataFrame(events)\n",
    "    \n",
    "    eventproperties = []\n",
    "    for eventpropertie in root.iterfind('*[@{http://www.omg.org/XMI}id][@docTimeRel]'):\n",
    "        eventproperties.append(dict(eventpropertie.attrib))\n",
    "    eventproperties_df = pandas.DataFrame(eventproperties)\n",
    "    \n",
    "    merged_df = mentions_df.add_suffix('_1')\\\n",
    "        .merge(right=concepts_df, left_on='ontologyConceptArr_1', right_on='{http://www.omg.org/XMI}id')\\\n",
    "        .merge(right=events_df, left_on='event_1', right_on='{http://www.omg.org/XMI}id')\\\n",
    "        .merge(right=eventproperties_df, left_on='properties', right_on='{http://www.omg.org/XMI}id')\n",
    "    \n",
    "#     # unique cui and tui per event IDEA: consider keeping all\n",
    "#     merged_df = merged_df.drop_duplicates(subset=['event', 'cui', 'tui'])\n",
    "    \n",
    "    # merge polarity of the *mention and the cui\n",
    "    merged_df['polaritycui'] = merged_df['polarity_1'] + merged_df['cui']\n",
    "    \n",
    "    # extract relations\n",
    "    textrelations = []\n",
    "    for tr in root.iterfind('*[@{http://www.omg.org/XMI}id][@category][@arg1][@arg2]'):\n",
    "        textrelations.append(dict(tr.attrib))\n",
    "    textrelations_df = pandas.DataFrame(textrelations)\n",
    "    \n",
    "    relationarguments = []\n",
    "    for relationargument in root.iterfind('*[@{http://www.omg.org/XMI}id][@argument][@role]'):\n",
    "        relationarguments.append(dict(relationargument.attrib))\n",
    "    relationarguments_df = pandas.DataFrame(relationarguments)    \n",
    "    \n",
    "    # merge relations\n",
    "    merged_relations_df = textrelations_df\\\n",
    "        .merge(right=relationarguments_df.add_suffix('_a1'), left_on='arg1', right_on='{http://www.omg.org/XMI}id_a1')\\\n",
    "        .merge(right=relationarguments_df.add_suffix('_a2'), left_on='arg2', right_on='{http://www.omg.org/XMI}id_a2')\\\n",
    "        .merge(right=merged_df.add_suffix('_1'), left_on='argument_a1', right_on='{http://www.omg.org/XMI}id_1_1')\\\n",
    "        .merge(right=merged_df.add_suffix('_2'), left_on='argument_a1', right_on='{http://www.omg.org/XMI}id_1_2')\n",
    "    \n",
    "    merged_relations_df['relations'] = merged_relations_df['polaritycui_1'] + merged_relations_df['category'] + merged_relations_df['polaritycui_2']\n",
    "\n",
    "    # return as a string\n",
    "    return ' '.join(list(merged_relations_df['relations']))\n",
    "\n",
    "X = []\n",
    "\n",
    "# key function for sorting the files according to the integer of the filename\n",
    "def key_fn(x):\n",
    "    i = x.split(\".\")[0]\n",
    "    if i != \"\":\n",
    "        return int(i)\n",
    "    return None\n",
    "\n",
    "for f in sorted(os.listdir(input_directory), key=key_fn):  # for each file in the input directory\n",
    "    if f.endswith(\".xmi\"):\n",
    "        fpath = os.path.join(input_directory, f)\n",
    "        print f\n",
    "        # parse file and append as a dataframe to x_df\n",
    "        try:\n",
    "            X.append(ctakes_xmi_to_df(fpath))\n",
    "        except:\n",
    "            X.append('NaN')\n",
    "\n",
    "X = numpy.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load annotations and classification data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and parse csv file\n",
    "data = pandas.read_csv(input_filename)\n",
    "# data = data[0:100]  # for testing\n",
    "# X = X[0:100]  # for testing\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups: the subject ids\n",
    "# used in order to ensure that \n",
    "# \"patientsâ€™ notes stay within the set, so that all discharge notes in the \n",
    "# test set are from patients not previously seen by the model.\" Gehrmann17.\n",
    "groups_df = data.filter(items=['subject.id']) \n",
    "groups = groups_df.as_matrix()\n",
    "# y: the annotated classes\n",
    "y_df = data.filter(items=conditions) # filter the conditions\n",
    "y = y_df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, groups.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of classifiers (sklearn estimators)\n",
    "classifiers = collections.OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    pattern = r'[\\s]+'  # match any sequence of whitespace characters\n",
    "    repl = r' '  # replace with space\n",
    "    temp_text = re.sub(pattern, repl, text)\n",
    "    return temp_text.lower().split(' ')  # lower-case and split on space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_models = [\n",
    "#     ('logistic_regression', LogisticRegression(random_state=0)),\n",
    "#     (\"random_forest\", RandomForestClassifier(random_state=0)),\n",
    "#     (\"naive_bayes\", MultinomialNB()),\n",
    "#     (\"svm_linear\", SVC(kernel=\"linear\", random_state=0)),\n",
    "    (\"gradient_boosting\", GradientBoostingClassifier(random_state=0)),\n",
    "]\n",
    "\n",
    "# BoW\n",
    "representation_models = [('ctakes_timegram_bow_tfidf', TfidfVectorizer(tokenizer=tokenizer, min_df=5))]  # IDEA: Use Tfidf on normal BoW model aswell?\n",
    "\n",
    "# cross product of representation models and prediction models\n",
    "# save to classifiers as pipelines of rep. model into pred. model\n",
    "for rep_model, pred_model in itertools.product(representation_models, prediction_models):\n",
    "    classifiers.update({  # add this classifier to classifiers dictionary\n",
    "        '{rep_model}_{pred_model}'.format(rep_model=rep_model[0], pred_model=pred_model[0]):  # classifier name\n",
    "        Pipeline([rep_model, pred_model]),  # concatenate representation model with prediction model in a pipeline\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluation.run_evaluation(X=X, \n",
    "                                    y=y, \n",
    "                                    groups=groups,\n",
    "                                    conditions=conditions,\n",
    "                                    classifiers=classifiers,\n",
    "                                    n_splits=n_splits, \n",
    "                                    n_repeats=n_repeats, \n",
    "                                    n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results        \n",
    "results_df = pandas.DataFrame(results)\n",
    "results_df.to_csv(results_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load results for plotting\n",
    "# import pandas\n",
    "# results = pandas.read_csv('output/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save\n",
    "axs = results_df.groupby('name').boxplot(column='AUROC', by='condition', rot=90, figsize=(10,10))\n",
    "for ax in axs:\n",
    "    ax.set_ylim(0,1)\n",
    "\n",
    "plt.savefig(plot_filename_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save\n",
    "axs = results_df.groupby('condition').boxplot(column='AUROC', by='name', rot=90, figsize=(10,10))\n",
    "for ax in axs:\n",
    "    ax.set_ylim(0,1)\n",
    "\n",
    "plt.savefig(plot_filename_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
